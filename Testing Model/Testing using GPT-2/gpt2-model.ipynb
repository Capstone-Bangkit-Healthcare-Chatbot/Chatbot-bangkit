{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nlpaug\n!pip install torchsummary\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T15:59:49.108781Z","iopub.execute_input":"2024-11-25T15:59:49.109427Z","iopub.status.idle":"2024-11-25T16:00:15.067736Z","shell.execute_reply.started":"2024-11-25T15:59:49.109389Z","shell.execute_reply":"2024-11-25T16:00:15.066863Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.2.2)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.32.3)\nCollecting gdown>=4.0.0 (from nlpaug)\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.15.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown, nlpaug\nSuccessfully installed gdown-5.2.0 nlpaug-1.1.11\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport os\nimport torch\nfrom datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:00:15.069610Z","iopub.execute_input":"2024-11-25T16:00:15.069894Z","iopub.status.idle":"2024-11-25T16:00:31.075941Z","shell.execute_reply.started":"2024-11-25T16:00:15.069866Z","shell.execute_reply":"2024-11-25T16:00:31.075248Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Cleaning Data","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\ndf = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\nprint(\"Data Sample\")\nprint(df.head())\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\ntotal_duplicates = df.duplicated(['question'], keep=False)\nprint(f\"Total duplicates in 'question' column: {total_duplicates.sum()}\")\nduplicates = df.duplicated()\nprint(f\"Number of duplicate rows: {duplicates.sum()}\")\ndf = df.drop_duplicates()\ndf.reset_index(drop=True, inplace=True)\nprint(\"Table Info\")\nprint(df.info())\ndf = df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)\ndf = df.drop_duplicates(subset='answer', keep='first').reset_index(drop=True)\ndf.dropna(inplace=True)\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\nprint(df.info())\ndf['question'] = df['question'].fillna('')\ndf['answer'] = df['answer'].fillna('')\ndf['prompt'] = df['question'] + ' ' + df['answer']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:00:31.076985Z","iopub.execute_input":"2024-11-25T16:00:31.077702Z","iopub.status.idle":"2024-11-25T16:00:31.580456Z","shell.execute_reply.started":"2024-11-25T16:00:31.077661Z","shell.execute_reply":"2024-11-25T16:00:31.579452Z"}},"outputs":[{"name":"stdout","text":"Data Sample\n                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer           source  \\\n0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n\n  focus_area  \n0   Glaucoma  \n1   Glaucoma  \n2   Glaucoma  \n3   Glaucoma  \n4   Glaucoma  \nNull Value Data\nquestion       0\nanswer         5\nsource         0\nfocus_area    14\ndtype: int64\nTotal duplicates in 'question' column: 2319\nNumber of duplicate rows: 48\nTable Info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16364 entries, 0 to 16363\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   question    16364 non-null  object\n 1   answer      16359 non-null  object\n 2   source      16364 non-null  object\n 3   focus_area  16350 non-null  object\ndtypes: object(4)\nmemory usage: 511.5+ KB\nNone\nNull Value Data\nquestion      0\nanswer        0\nsource        0\nfocus_area    0\ndtype: int64\n<class 'pandas.core.frame.DataFrame'>\nIndex: 14457 entries, 0 to 14463\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   question    14457 non-null  object\n 1   answer      14457 non-null  object\n 2   source      14457 non-null  object\n 3   focus_area  14457 non-null  object\ndtypes: object(4)\nmemory usage: 564.7+ KB\nNone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Architecting Model","metadata":{}},{"cell_type":"code","source":"# Split data into training and validation sets\ntrain_data, val_data = train_test_split(df['prompt'], test_size=0.1, random_state=42)\n\n# Save the train and validation data to text files\ntrain_data.to_csv('/kaggle/working/train.txt', index=False, header=False)\nval_data.to_csv('/kaggle/working/val.txt', index=False, header=False)\n\n# Load pre-trained GPT-2 tokenizer \ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\n# Use the PyTorch version of GPT-2\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Tokenize datasets\ndef tokenize_function(examples):\n    return tokenizer(examples, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n# Define a custom compute_metrics function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    # Flatten both predictions and labels to ignore padding\n    flattened_predictions = predictions.flatten()\n    flattened_labels = labels.flatten()\n    mask = flattened_labels != -100  # Exclude padding (-100 is used by Hugging Face)\n    filtered_predictions = flattened_predictions[mask]\n    filtered_labels = flattened_labels[mask]\n    accuracy = accuracy_score(filtered_labels, filtered_predictions)\n    return {\"accuracy\": accuracy}\n# Create dataset for training and evaluation\ntrain_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path='/kaggle/working/train.txt',\n    block_size=128\n)\n\nval_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path='/kaggle/working/val.txt',\n    block_size=128\n)\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# Set training arguments\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/chatbot_model',         \n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    logging_steps=10,\n    save_steps=4100,  \n    prediction_loss_only=False,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\ntrainer.train()\n# eval_results = trainer.evaluate()\nmodel.save_pretrained('/kaggle/working/chatbot_model.h5')\ntokenizer.save_pretrained('/kaggle/working/chatbot_model.h5')\n# print(f\"Validation Results: {eval_results}\")\nmodel_path = \"./model.h5\"\ntorch.save(model.state_dict(), model_path)\nprint(\"Model and tokenizer saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:00:31.582136Z","iopub.execute_input":"2024-11-25T16:00:31.582443Z","iopub.status.idle":"2024-11-25T16:21:07.413053Z","shell.execute_reply.started":"2024-11-25T16:00:31.582415Z","shell.execute_reply":"2024-11-25T16:21:07.411919Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eb1a9e0f9c64347aabf215446a9de29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3203e5bdb2405a9e05dd1b32468439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2800cce789094106b319f547d4b8cbea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37abf3bfc5e6426294c6207f728427c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e3f62b9f2fd41af9e0bc2061af62f4c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af017b00091d4544a5f63f4d1d5baa03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1ccfc2056e4a1dabb1833a9e37c062"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4104' max='4104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4104/4104 20:04, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.043000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.805600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.586800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.575600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.544900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.309600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.471600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.402200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.313300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.274800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.287000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.291200</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.203200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.149600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.163000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.188200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.108000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.219100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.131400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.215000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.142300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.045900</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.231000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.141800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.165700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.097900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.095900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.161300</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.102800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.981800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.074800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.039600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>2.248600</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.089200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.953600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.009000</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.964200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.124900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>2.111900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.080300</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>2.096700</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.906200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>2.050300</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>2.008400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.999200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.149800</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.967600</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>2.121100</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.940100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.929800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.938300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.882000</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>2.003300</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.952400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.858100</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.946100</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>2.090000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.853500</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>2.105100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.916100</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>2.110700</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>2.014000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.928100</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.901700</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.986000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.922500</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.946500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>2.069000</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.970400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.047500</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.921500</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.907400</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.854600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.819600</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.666300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.848200</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.978800</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.839100</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.863300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.929300</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.981700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>2.018900</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.849400</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>2.077200</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.927100</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.793400</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.928900</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.848700</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>1.818400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.820600</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>1.830900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.836600</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>1.862200</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.920100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.851400</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.746000</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>1.845900</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>1.726800</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>1.966800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.792500</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>1.891000</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>1.702100</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>1.617300</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>1.988100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>1.883000</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>1.768500</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>1.830400</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>1.879900</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>1.860500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.886600</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>1.763100</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>1.718600</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>1.852300</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>1.787300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>2.003900</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>1.773000</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>1.847000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>2.012900</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>1.891700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.648800</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>1.937900</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>1.918600</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>1.949200</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>1.659900</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.907800</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>1.868200</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>1.914000</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>1.778900</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>1.598700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.717600</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>1.910900</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>1.868500</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>1.857700</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>1.917100</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>1.769900</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>1.476800</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>1.741400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>1.829000</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>1.752400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.808300</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>1.871500</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>1.742400</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>1.768200</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>1.804200</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>1.880400</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>1.781600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>1.775800</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>1.728000</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>1.936800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.814200</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>1.809500</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>1.635600</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>1.847200</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>1.742100</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>1.727600</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>1.814000</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>1.795400</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>1.817100</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>1.798000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.809200</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>1.688000</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>1.920500</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>1.830200</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>1.898500</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>1.708900</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>1.692800</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>1.759000</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>1.740800</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>1.941900</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.770200</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>1.811100</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>1.721300</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>1.688000</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>1.869900</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>1.788100</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>1.696100</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>1.793700</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>1.690800</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>1.963400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.017000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>1.884900</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>1.911700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>1.767000</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>1.814400</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>1.801500</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>1.762100</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>1.925400</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>1.915900</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>1.718800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.642900</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>1.604100</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>1.708100</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>1.604600</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>1.798700</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>1.633400</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>1.752400</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>1.846200</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>1.813100</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>1.724200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.585000</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>1.673100</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>1.756100</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>1.695500</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>1.561400</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>1.780500</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>1.848200</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>1.894500</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>1.934600</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>1.758700</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.839700</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>1.702400</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>1.821900</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>1.803900</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>1.667800</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>1.892400</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>1.687500</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>1.641700</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>1.939100</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>1.814800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.787400</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>1.723900</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>1.803600</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>1.811900</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>1.597100</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>1.959700</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>1.801300</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>1.739900</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>1.799400</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>1.769100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.550200</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>1.879600</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>1.722400</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>1.741200</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>1.644400</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>1.831700</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>1.839800</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>1.539600</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>1.793200</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>1.623800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.471700</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>1.699800</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>1.884700</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>1.683200</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>1.696000</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>1.701400</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>1.723500</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>1.659400</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>1.765200</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>1.552300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.618800</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>1.623200</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>1.886700</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>1.765500</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>1.751500</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>1.776500</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>1.662200</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>1.886800</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>1.809300</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>1.635800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.712200</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>1.754100</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>1.805000</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>1.807000</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>1.712700</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>1.812100</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>1.689200</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>1.785900</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>1.670600</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>1.879100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>1.857800</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>1.630800</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>1.774900</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>1.658000</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>1.893400</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>1.771300</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>1.766000</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>1.723400</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>1.819100</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>1.777600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.645200</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>1.774700</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>1.804900</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>1.732200</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>1.661300</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>1.843100</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>1.691100</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>1.748200</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>1.556800</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>1.718000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>1.845700</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>1.634700</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>1.696000</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>1.613000</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>1.687400</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>1.741600</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>1.636100</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>1.809000</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>1.780200</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>1.892900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.639700</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>1.777400</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>1.700300</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>1.795400</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>1.801100</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>1.641100</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>1.727000</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>1.729300</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>1.547400</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>1.629800</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>1.643000</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>1.580900</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>1.837900</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>1.759800</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>1.770400</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>1.630700</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>1.681000</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>1.929700</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>1.623500</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>1.811300</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.682200</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>1.834500</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>1.626400</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>1.621500</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>1.729600</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>1.680800</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>1.826100</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>1.704800</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>1.748300</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>1.778200</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>1.770800</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>1.899000</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>1.814100</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>1.673500</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>1.626800</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>1.664600</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>1.563500</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>1.731700</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>1.701400</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>1.695600</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>1.723300</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>1.820100</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>1.736900</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>1.680700</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>1.494500</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>1.763200</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>1.824500</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>1.588600</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>1.692600</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>1.485900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.546800</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>1.611300</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>1.591700</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>1.746500</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>1.627800</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>1.741000</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>1.615900</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>1.750500</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>1.653200</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>1.710400</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.827800</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>1.764300</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>1.665700</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>1.659400</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>1.828700</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>1.725900</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>1.842800</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>1.662900</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>1.762900</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>1.758800</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>1.612000</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>1.655900</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>1.584400</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>1.748600</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>1.787100</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>1.800200</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>1.848100</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>1.429300</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>1.799800</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>1.734600</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>1.795500</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>1.707700</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>1.688000</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>1.612400</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>1.824500</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>1.764400</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>1.595100</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>1.630000</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>1.699100</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>1.740400</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>1.702100</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>1.745600</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>1.834700</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>1.699100</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>1.719300</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>1.701700</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>1.800500</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>1.668700</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>1.736500</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>1.710000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.784200</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>1.701300</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>1.747400</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>1.683400</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>1.752400</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>1.730600</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>1.660800</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>1.546800</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>1.658100</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>1.890200</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>1.815000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Model and tokenizer saved successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Testing Using Inputs","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained('/kaggle/working/chatbot_model.h5')\ntokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/chatbot_model.h5')\n\n# Function to generate chatbot response\ndef generate_response(prompt):\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n    outputs = model.generate(inputs, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Example usage\nprompt = \"what is flu?\"\nresponse = generate_response(prompt)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T16:37:28.307858Z","iopub.execute_input":"2024-11-25T16:37:28.308644Z","iopub.status.idle":"2024-11-25T16:37:33.014761Z","shell.execute_reply.started":"2024-11-25T16:37:28.308610Z","shell.execute_reply":"2024-11-25T16:37:33.013823Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"what is flu? Flu is a cold-like illness that causes fever, headache, and shortness of breath. It can also cause fever and other symptoms. Flu can be caused by a variety of causes, including:\n  \n- cold weather  - cold temperatures - flu\n - flu-related illnesses  -- cold illnesses\n flu  flu - other illnesses, such as pneumonia\n influenza  influenza - pneumonia  cold flu, flu or cold viruses\n cold virus  other diseases,such as cold diseases cold cold disease cold illness other cold infections\n or flu cold infection cold or influenza\n and cold  or  cough  and flu cough\nor  a cough and  coughing\nand  an infection\n\"What are the symptoms\n","output_type":"stream"}],"execution_count":9}]}