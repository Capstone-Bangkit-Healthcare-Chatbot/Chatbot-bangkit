{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3723297,"sourceType":"datasetVersion","datasetId":2222120},{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565},{"sourceId":9941146,"sourceType":"datasetVersion","datasetId":6112205},{"sourceId":176166,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":149994,"modelId":172485}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"import torch\nimport wandb\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments\nfrom transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.amp import autocast  \n\nfrom collections import defaultdict\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-23T19:01:02.776590Z","iopub.execute_input":"2024-11-23T19:01:02.777268Z","iopub.status.idle":"2024-11-23T19:01:24.610297Z","shell.execute_reply.started":"2024-11-23T19:01:02.777229Z","shell.execute_reply":"2024-11-23T19:01:24.609384Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Cleaning Data","metadata":{}},{"cell_type":"code","source":"#Load Data\ndf = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\n\n# Sampel Data\nprint(\"Data Sample\")\nprint(df.head())\n\n#Null value\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\nduplicates = df.duplicated(['question'], keep=False).sum()\nprint(f\"Total duplicates in 'question' column: {duplicates}\")\n\n# Check for duplicate rows\n\nduplicates = df.duplicated()\nprint(f\"Number of duplicate rows: {duplicates.sum()}\")\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Reset the index after removing duplicates\ndf.reset_index(drop=True, inplace=True)\n\n#Delete Unused column\ndf = df.drop(columns=['source', 'focus_area'])\n\n#Table Info\nprint(\"Table Info\")\nprint(df.info())\n\n# Apply the function\ndf = df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)\ndf = df.drop_duplicates(subset='answer', keep='first').reset_index(drop=True)\n\n#Drop rows with null values\ndf.dropna(inplace=True)\n\n#Checking again of null values\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\n#Checking again of the data info\nprint(df.info())\n\n#Check for Unique Data\nprint(f\"Unique questions: {df['question'].nunique()}\")\nprint(f\"Unique answers: {df['answer'].nunique()}\")\n\ndf['question'] = df['question'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\ndf['answer'] = df['answer'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-23T19:01:24.611936Z","iopub.execute_input":"2024-11-23T19:01:24.613135Z","iopub.status.idle":"2024-11-23T19:01:26.087587Z","shell.execute_reply.started":"2024-11-23T19:01:24.613094Z","shell.execute_reply":"2024-11-23T19:01:26.086730Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Data Sample\n                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer           source  \\\n0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n\n  focus_area  \n0   Glaucoma  \n1   Glaucoma  \n2   Glaucoma  \n3   Glaucoma  \n4   Glaucoma  \nNull Value Data\nquestion       0\nanswer         5\nsource         0\nfocus_area    14\ndtype: int64\nTotal duplicates in 'question' column: 2319\nNumber of duplicate rows: 48\nTable Info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16364 entries, 0 to 16363\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  16364 non-null  object\n 1   answer    16359 non-null  object\ndtypes: object(2)\nmemory usage: 255.8+ KB\nNone\nNull Value Data\nquestion    0\nanswer      0\ndtype: int64\n<class 'pandas.core.frame.DataFrame'>\nIndex: 14463 entries, 0 to 14463\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  14463 non-null  object\n 1   answer    14463 non-null  object\ndtypes: object(2)\nmemory usage: 339.0+ KB\nNone\nUnique questions: 14463\nUnique answers: 14463\n                                 question  \\\n0                what is (are) glaucoma ?   \n1                  what causes glaucoma ?   \n2     what are the symptoms of glaucoma ?   \n3  what are the treatments for glaucoma ?   \n4          who is at risk for glaucoma? ?   \n\n                                              answer  \n0  glaucoma is a group of diseases that can damag...  \n1  nearly 2.7 million people have glaucoma, a lea...  \n2  symptoms of glaucoma glaucoma can develop in o...  \n3  although open-angle glaucoma cannot be cured, ...  \n4  anyone can develop glaucoma. some people are a...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Architecting Model","metadata":{}},{"cell_type":"code","source":"# callback to track metrics\nclass MetricsCallback(TrainerCallback):\n    def __init__(self):\n        self.metrics = defaultdict(list)\n        \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            for key, value in logs.items():\n                if isinstance(value, (int, float)):\n                    self.metrics[key].append(value)\n    \n    def plot_metrics(self):\n        plt.figure(figsize=(15, 10))\n        \n        # Plot training and validation loss\n        plt.subplot(2, 1, 1)\n        if 'loss' in self.metrics:\n            plt.plot(self.metrics['loss'], label='Training Loss')\n        if 'eval_loss' in self.metrics:\n            # Interpolate eval_loss to match training loss points\n            eval_steps = len(self.metrics['eval_loss'])\n            train_steps = len(self.metrics['loss'])\n            eval_indices = np.linspace(0, train_steps-1, eval_steps)\n            plt.plot(eval_indices, self.metrics['eval_loss'], label='Validation Loss', marker='o')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Training Steps')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.grid(True)\n\n        # Plot exact match score\n        plt.subplot(2, 1, 2)\n        if 'eval_exact_match' in self.metrics:\n            plt.plot(self.metrics['eval_exact_match'], label='Exact Match Score', marker='o')\n        if 'eval_bleu_score' in self.metrics:\n            plt.plot(self.metrics['eval_bleu_score'], label='BLEU Score', marker='o')\n        plt.title('Evaluation Metrics')\n        plt.xlabel('Evaluation Steps')\n        plt.ylabel('Score')\n        plt.legend()\n        plt.grid(True)\n\n        plt.tight_layout()\n        plt.savefig('training_metrics.png')\n        plt.close()\n\n# Load T5-small model and tokenizer\nmodel_name = \"t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Preprocess function for seq2seq task\ndef preprocess_function(batch):\n    inputs = [f\"question: {q}\" for q in batch[\"question\"]]\n    targets = [f\"{a}\" for a in batch[\"answer\"]]\n    \n    model_inputs = tokenizer(\n        inputs, \n        max_length=128, \n        truncation=True, \n        padding=True,  \n        return_tensors='pt'\n    )\n    labels = tokenizer(\n        targets, \n        max_length=128, \n        truncation=True, \n        padding=True,\n        return_tensors='pt'\n    )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"]\n    return model_inputs\n\n# Train-test split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Preprocess datasets\ntrain_dataset = train_dataset.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=train_dataset.column_names,\n    num_proc=4\n)\nval_dataset = val_dataset.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=val_dataset.column_names,\n    num_proc=4\n)\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"steps\",\n    eval_steps=300,\n    save_steps=300,\n    save_total_limit=3,\n    learning_rate=5e-5,  \n    num_train_epochs=10, \n    per_device_train_batch_size=16,  \n    per_device_eval_batch_size=16,\n    lr_scheduler_type=\"cosine\",  \n    warmup_ratio=0.1,  \n    weight_decay=0.01,\n    predict_with_generate=True,\n    fp16=True,\n    gradient_accumulation_steps=4, \n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\",\n    max_grad_norm=1.0,              \n    dataloader_num_workers=4,      \n    ddp_find_unused_parameters=False,\n    group_by_length=True,\n)\n\n# Label smoothing \nclass AdaptiveLabelSmoothingLoss(CrossEntropyLoss):\n    def __init__(self, smoothing=0.1, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.smoothing = smoothing\n        self.confidence = 1.0 - smoothing\n        \n    def forward(self, input, target):\n        log_prob = torch.nn.functional.log_softmax(input, dim=-1)\n        weight = input.new_ones(input.size()) * \\\n                 self.smoothing / (input.size(-1) - 1.)\n        weight.scatter_(-1, target.unsqueeze(-1), self.confidence)\n        return torch.mean(torch.sum(-weight * log_prob, dim=-1))\n\n# data collator with length-based batching\nclass SmartDataCollator(DataCollatorForSeq2Seq):\n    def __call__(self, features):\n        # Sort by length for more efficient batching\n        features = sorted(features, key=lambda x: len(x['input_ids']))\n        return super().__call__(features)\n\ndata_collator = SmartDataCollator(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True,\n)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    # Decode predictions and labels\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Ensure non-empty decoded results\n    if not decoded_preds or not decoded_labels:\n        print(\"Empty predictions or labels detected.\")\n        return {\"exact_match\": 0, \"bleu_score\": 0}\n\n    # Calculate Exact Match\n    exact_matches = [int(pred.strip() == label.strip()) for pred, label in zip(decoded_preds, decoded_labels)]\n    exact_match_score = np.mean(exact_matches) * 100\n\n    # Calculate BLEU Score\n    try:\n        bleu_score = corpus_bleu(\n            [[label.split()] for label in decoded_labels],\n            [pred.split() for pred in decoded_preds]\n        ) * 100\n    except ZeroDivisionError:\n        bleu_score = 0\n\n    return {\"exact_match\": exact_match_score, \"bleu_score\": bleu_score}\n\n\n# Trainer with metrics callback\nmetrics_callback = MetricsCallback()\n\n# Initialize Trainer\nmodel.config.label_smoothing = 0.1\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[metrics_callback]\n)\n\n# model summary function\ndef print_detailed_model_summary(model, show_architecture=False):\n    print(\"\\nModel Summary:\")\n    print(\"-\" * 50)\n    print(f\"Model Type: T5-base\")\n    \n    # Count parameters by layer type\n    layer_params = defaultdict(int)\n    for name, param in model.named_parameters():\n        layer_type = name.split('.')[0]\n        layer_params[layer_type] += param.numel()\n    \n    # Print total parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"Total Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\")\n    print(f\"Non-trainable Parameters: {total_params - trainable_params:,}\")\n    \n    # Print parameters by layer type\n    print(\"\\nParameters by Layer Type:\")\n    for layer_type, params in layer_params.items():\n        print(f\"{layer_type}: {params:,} parameters ({params/total_params*100:.2f}%)\")\n    \n    if show_architecture:\n        # Print model architecture\n        print(\"\\nModel Architecture:\")\n        print(\"-\" * 50)\n        for name, module in model.named_modules():\n            if len(name) > 0:  # Skip the root module\n                print(f\"{name}: {module.__class__.__name__}\")\n        print(\"-\" * 50)\n    else:\n        print(\"\")\n\n# Training workflow\nprint_detailed_model_summary(model)\ntrainer.train()\nmetrics_callback.plot_metrics()\n\n# Final evaluation\n# test_results = trainer.evaluate(test_dataset)\n# print(\"\\nTest Set Results:\")\n# print(json.dumps(test_results, indent=2))\n\n# Save the model\ntrainer.save_model(\"./t5_chatbot_model\")\ntokenizer.save_pretrained(\"./t5_chatbot_tokenizer\")\nmodel_path = \"./t5_chatbot_model.h5\"\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-23T19:01:26.089154Z","iopub.execute_input":"2024-11-23T19:01:26.089790Z","iopub.status.idle":"2024-11-23T20:04:51.018956Z","shell.execute_reply.started":"2024-11-23T19:01:26.089746Z","shell.execute_reply":"2024-11-23T20:04:51.017846Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2176bfd01f3422ab862fc0082e1fe04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42e942be4884f4793bf67178bc79f18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41ddebd43a1441bb379b6f7df1fcac2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"573f23fb8ddb4247937ff638aac07651"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3de01076ddd24b08885dd0c6e63c02e3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/11570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93aa3372b7eb40579963b13661f5a5e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/2893 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d54ad422808f4822b9bc30a6864a7b07"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nModel Summary:\n--------------------------------------------------\nModel Type: T5-base\nTotal Parameters: 222,903,552\nTrainable Parameters: 222,903,552\nNon-trainable Parameters: 0\n\nParameters by Layer Type:\nshared: 24,674,304 parameters (11.07%)\nencoder: 84,954,240 parameters (38.11%)\ndecoder: 113,275,008 parameters (50.82%)\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [900/900 1:02:56, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Exact Match</th>\n      <th>Bleu Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>300</td>\n      <td>1.755900</td>\n      <td>1.620738</td>\n      <td>0.000000</td>\n      <td>0.151595</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.620900</td>\n      <td>1.550666</td>\n      <td>0.034566</td>\n      <td>0.150061</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.596900</td>\n      <td>1.540370</td>\n      <td>0.034566</td>\n      <td>0.146597</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Testing using Input","metadata":{}},{"cell_type":"code","source":"# Load the trained T5 model and tokenizer\nmodel_path = \"/kaggle/working/t5_chatbot_model\"\ntokenizer_path = \"/kaggle/working/t5_chatbot_tokenizer\"\n\ntokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\nmodel.eval() \n\ndef generate_response(question):\n    input_ids = tokenizer(f\"question: {question} </s>\", return_tensors=\"pt\").input_ids.to(model.device)\n    outputs = model.generate(\n        input_ids,\n        max_length=128,\n        num_beams=5,  \n        no_repeat_ngram_size=2,  \n        top_k=50,  \n        top_p=0.95,  \n        temperature=1.0  \n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example usage\nresponse = generate_response(\"What causes diabetes ?\")\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-11-23T20:11:09.665983Z","iopub.execute_input":"2024-11-23T20:11:09.666372Z","iopub.status.idle":"2024-11-23T20:11:22.592862Z","shell.execute_reply.started":"2024-11-23T20:11:09.666339Z","shell.execute_reply":"2024-11-23T20:11:22.591965Z"},"trusted":true},"outputs":[{"name":"stdout","text":"what causes diabetes? the exact cause of diabetes is unknown.\n","output_type":"stream"}],"execution_count":7}]}